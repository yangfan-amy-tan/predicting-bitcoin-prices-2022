{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-eEvm7PqCuo"
   },
   "source": [
    "\n",
    "# Calculate daily sentiment scores with tweet metrics considered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qa1qcwiskBN"
   },
   "source": [
    "WARNING: THIS NOTEBOOK TOOK ABOUT 2 HOURS TO RUN! You might want to skip running this and use the generated dataset to do analysis in the next notebook. <br>\n",
    "<br>\n",
    "It takes a csv with tweets abut Bitcoin as an input. <br>\n",
    "Then it calculates sentiment for each tweet in the file, taking also into account the likes, retweets and replies each of the tweet has. <br>\n",
    "It generates a dataset of daily sentiment scores: \"date_and_score.csv\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNqk4-XRtPsC"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyI4PizgmwQJ",
    "outputId": "c321c782-49e8-4620-bc46-398945b6864b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from statistics import mean\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzkxCddaqgGd"
   },
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjRzNh5BnTXX"
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "analyzed_tweets = 0\n",
    "cleaned_tweets = 0\n",
    "\n",
    "def find_sentiment(text):\n",
    "    global sia\n",
    "    global analyzed_tweets\n",
    "    print(\" \"*30, end = \"\\r\")\n",
    "    print(\"Analyzing tweet:\", analyzed_tweets + 1, end = \"\\r\")\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    analyzed_tweets += 1\n",
    "    return [sentiment[\"neg\"], sentiment[\"neu\"], sentiment[\"pos\"], sentiment[\"compound\"]]\n",
    "\n",
    "\n",
    "def consider_tweet_metrics(text, replies, likes, retweets):  #maybe should also consider favorites\n",
    "  sentiment = find_sentiment(text)\n",
    "  score = sentiment[-1]\n",
    "  subjectivity = find_subjectivity(text) # [0, 1], whereas 0 means very factual and objective and 1 means highly subjective opinion\n",
    "  replies += 1\n",
    "  likes += 1\n",
    "  retweets += 1\n",
    "  return sentiment + [score * replies * likes * retweets * (1 - subjectivity**4)]\n",
    "\n",
    "def clean_text(text):\n",
    "    global cleaned_tweets\n",
    "    print(\" \"*30, end = \"\\r\")\n",
    "    print(\"CLeaning tweet:\", cleaned_tweets + 1, end = \"\\r\")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text) #remove punctuation, URL and @\n",
    "\n",
    "    #stop = stopwords.words('english')\n",
    "    stop = nltk.corpus.stopwords.words(['english'])\n",
    "    words = text.split()\n",
    "    can_use = []\n",
    "    for word in words:\n",
    "      if word not in stop:\n",
    "        can_use.append(word)\n",
    "    lemmatised = [lem.lemmatize(t) for t in can_use]\n",
    "\n",
    "    cleaned_tweets += 1\n",
    "    return \" \".join(lemmatised)\n",
    "\n",
    "def find_subjectivity(text):\n",
    "  return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqDBnZEmqpdR"
   },
   "source": [
    "## Register start time for timing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_euMjUjnuPw",
    "outputId": "9f7a970b-6a38-43a5-e3e8-5e107fb2479c"
   },
   "outputs": [],
   "source": [
    "script_start = time.time()\n",
    "print(datetime.datetime.fromtimestamp(script_start).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wl7f1-_qxnf"
   },
   "source": [
    "## Read the csv containing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "BYTbchDtnVIW",
    "outputId": "b7f9780d-ea9b-4ad3-f10a-c0116e894513"
   },
   "outputs": [],
   "source": [
    "f = \"tweets.csv\"\n",
    "nrows= 20843765\n",
    "sep = \";\"\n",
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2019-11-22\"\n",
    "\n",
    "df_full = pd.read_csv(f, nrows=nrows, sep=sep) # specifying the nrows parameter seems to help avoid running out of memory when reading the csv\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJXW_fZMq-EG"
   },
   "source": [
    "## Force data types and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmi4gwZsoDIq"
   },
   "outputs": [],
   "source": [
    "df_full[\"replies\"] = pd.to_numeric(df_full[\"replies\"], errors='coerce')\n",
    "df_full[\"likes\"] = pd.to_numeric(df_full[\"likes\"], errors='coerce')\n",
    "df_full[\"retweets\"] = pd.to_numeric(df_full[\"retweets\"], errors='coerce')\n",
    "\n",
    "df_full = df_full.dropna(subset = ['timestamp','text']) #instances without date or text are useless, if necessary uncheck the ['test']\n",
    "date_filter=\"2016-|2017-|2018-|2019-\"\n",
    "df_full = df_full[df_full['timestamp'].str.contains(date_filter) == True] #to assert the correctness of date\n",
    "df_full = df_full.sort_values(by = 'timestamp')\n",
    "\n",
    "df_full['timestamp'] = df_full['timestamp'].apply(lambda x: x[:10]) # Only leave the first 10 characters of date field to discard time of the day and only leave date\n",
    "\n",
    "\n",
    "df=df_full\n",
    "#df = df_full.sample(frac = 0.001, replace = False, random_state = 1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9NF_JnHrIFN"
   },
   "source": [
    "## Clean the text and run sentiment analysis on all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93Q89yM7o7oC"
   },
   "outputs": [],
   "source": [
    "df['text'] = df.apply(lambda row : clean_text(row['text']), axis = 1)  #clean the text first\n",
    "#df['score'] = df.apply(lambda row : consider_tweet_metrics(row['text'], row['replies'], row['likes'], row['retweets']), axis = 1) #find the score for each left istance\n",
    "df['Sentiment_data'] = df.apply(lambda row : consider_tweet_metrics(row['text'], row['replies'], row['likes'], row['retweets']), axis = 1) #find the score for each left istance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plzs-lWWsE2m"
   },
   "source": [
    "## Aggregate sentiment analysis results to daily bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q17h9a0zpLfx"
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(start = start_date, end = end_date) #so that all the dates are definitely filled (to avoid gaps while merging with the price dataframe)\n",
    "dates = dates.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "avg_sentiment = []\n",
    "tweets_volume = []\n",
    "neg_sent_proportion = []\n",
    "neu_sent_proportion = []\n",
    "pos_sent_proportion = []\n",
    "avg_replies = []\n",
    "avg_likes = []\n",
    "avg_retweets = []\n",
    "scores = []\n",
    "for date in dates:\n",
    "    #s_date = date.strftime('%Y-%m-%d')\n",
    "    current_dates = df[df[\"timestamp\"] == date]\n",
    "    avg_sentiment.append(current_dates[\"Sentiment_data\"].str.get(3).mean())\n",
    "    avg_replies.append(current_dates[\"replies\"].mean())\n",
    "    avg_likes.append(current_dates[\"likes\"].mean())\n",
    "    avg_retweets.append(current_dates[\"retweets\"].mean())\n",
    "    volume = current_dates.shape[0]\n",
    "    tweets_volume.append(volume)\n",
    "    neg_sent_proportion.append(current_dates[\"Sentiment_data\"].str.get(0).mean())\n",
    "    neu_sent_proportion.append(current_dates[\"Sentiment_data\"].str.get(1).mean())\n",
    "    pos_sent_proportion.append(current_dates[\"Sentiment_data\"].str.get(2).mean())\n",
    "    scores.append(current_dates[\"Sentiment_data\"].str.get(3).mean())\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "df_final[\"Date\"] = dates\n",
    "df_final[\"Avg_sentiment\"] = avg_sentiment\n",
    "df_final[\"Tweets_volume\"] = tweets_volume\n",
    "df_final[\"Neg_sent_proportion\"] = neg_sent_proportion\n",
    "df_final[\"Neu_sent_proportion\"] = neu_sent_proportion\n",
    "df_final[\"Pos_sent_proportion\"] = pos_sent_proportion\n",
    "df_final[\"Avg_replies\"] = avg_replies\n",
    "df_final[\"Avg_likes\"] = avg_likes\n",
    "df_final[\"Avg_retweets\"] = avg_retweets\n",
    "df_final[\"score\"] = np.array(scores)/np.nanmax(np.abs(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5htt72pwsQOJ"
   },
   "source": [
    "## Calculate moving averages of sentiment data to get trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdQjidukpY6M"
   },
   "outputs": [],
   "source": [
    "### CALCULATE MOVING AVERAGES, THESE MIGHT OR MIGHT NOT BE USED IN TTHE FINAL PREDICTION MODEL\n",
    "\n",
    "labels = [\"Avg_sentiment\", \"Tweets_volume\", \"Neg_sent_proportion\", \"Neu_sent_proportion\", \"Pos_sent_proportion\", \"Avg_replies\", \"Avg_likes\", \"Avg_retweets\", \"score\"]\n",
    "ma_periods = [2, 7, 21]\n",
    "\n",
    "for period in ma_periods:\n",
    "        current_df = df_final[labels]\n",
    "        moving_averages = current_df.rolling(period, min_periods=period).mean()\n",
    "        trends = pd.DataFrame()\n",
    "        for label in labels:\n",
    "          trends[f\"{period}_ma_trend_{label}\"] = current_df[label]/moving_averages[label] - 1\n",
    "        trends.dropna(axis=1, how='all')\n",
    "        for label in trends.columns:\n",
    "          if trends[label].describe()[\"count\"] > 0:\n",
    "            df_final[label] = trends[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baeNsO0kseHl"
   },
   "source": [
    "## Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCj12M3-pbjW"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv('date_and_score.csv', index=False)\n",
    "print(\"Total runtime: \", time.time() - script_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
